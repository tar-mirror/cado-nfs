###########################################################################
#     Parameter file for Cado-NFS
###########################################################################

# This file is a sample parameter file for a 91-digit gnfs input

# Anything after a # is a comment, until the end of the line. Any empty line
# is ignored. All quotes in the comments in this file are for emphasis and
# not part of the parameters.

# Each parameter should be on an individual line.
# The parameters form a tree structure. A parameter of the form
#   foo.bar.baz = 42
# is said to have path "foo.bar", key "baz", and value "42".

# Objects of the Python scripts that take parameters from the parameter file
# have a path associated with them; for example, the Sieving task uses path
# "tasks.sieve", and the las siever run in the Sieving task uses
# "tasks.sieve.las". When looking for a parameter, we start at the object's
# path, then work towards to root of the tree until the key is found.
# This means that, in principle, all parameters could be specified without a
# path, so long as no two parameters of the same key with different values
# occur. In this file we specify most parameters will full path to clarify
# which part of the computation they affect.
#
# Note that an automatically generated, and by construction comprehensive
# list of the recognized parameters can be obtained by starting (and
# quickly aborting, if needed) the cado-nfs.py script with the
# --verboseparam option. It will do a couple of things, but it will also
# print this list.

# Some variable substitution occurs in the values specified in the parameters:
# A string of the form "${FOO}" is substituted by the value of the shell
# environment variable "$FOO".
# A string of the form "$(foo)" is substituted by the value of the parameter
# with key "foo", where the search starts at the path where "$(foo)"
# occurred.

# Some parameters apply to all steps of the algorithm (General parameters
# below), and are related to the execution environment. Some other
# parameters control the many different tunables of the execution of the
# algorithm. Such parameters are relevant to only one of the steps of NFS,
# and are thus listed under the section related to the corresponding
# step. Each of the sections corresponding to one step in particular
# also contains a couple of parameters relative to the execution
# environment (for that step only).

# The cado-nfs.py script may be used as:
# cado-nfs.py <parametersfile> [<additional parameters> [...]]
# Additional parameters on the command line are specified in the same
# format as in this file, e.g., "foo.bar.baz=42", and override values found
# in the parameter file.

###########################################################################
# General parameters
###########################################################################

# General parameters are relevant to the behaviour of the cado-nfs.py script.

# - location and name of files -
name = c90
# All files created by cadofactor will be prefixed by this name.


# The other general parameters (N, sourcedir, builddir, tasks.workdir) in
# this section are commented out, because they do not make sense on a
# general basis (recall that this very file is also used as a baseline
# for factoring any 90-digit number).
#
# N: The number to be factored. This c90 here is given only as an
# example, and hence commented out. You may set a value for it in this
# file or override it on the command line
#
# adjust if needed
# N = 482940676938975075806475467716373950242726509723517010302524110187257004202892794033435637

# Adjust sourcedir to point to the root of your CADO source directory.
# This parameter is not actually used by cadofactor directly, but it is
# referenced as $(sourcedir) further down in this parameter file.
#
# adjust if needed
# sourcedir=${HOME}/cado-nfs

# Adjust builddir to point to the root of your CADO build directory
# Not used directly, either, but referenced, too.
#
# adjust if needed
# builddir=${HOME}/build/cado-nfs/

# workdir: the path of the working directory; if a relative path is
# specified, it is relative to the CWD from which cadofactor is run.
# All the output files of sieving, filtering, etc., are stored under the
# working directory.
# Note that there is no need to have the working directory located under
# the CADO source or build directory. In fact, it is advisable to keep it
# separate to facilitate deleting or backing-up the sources, executables,
# and factorization data individually.
#
# adjust if needed
# tasks.workdir = /tmp/

# For reference, here is the list of parameters found in various places
# within this file, which are to be adjusted in case the default values
# aren't fine. We consider that it is not meaningful for a parameter file
# to contain values for these, as the choice is really a matter of taste
# for the user.
# sourcedir=${HOME}/cado-nfs
# builddir=${HOME}/build/cado-nfs/
# tasks.workdir = /tmp/
# tasks.execpath=$(builddir)
# slaves.hostnames = localhost
# slaves.nrclients = 2
# slaves.scriptpath = $(sourcedir)
# slaves.downloadretry = 10
# slaves.niceness = 10		# nice level for the sieving jobs
# slaves.basepath = /tmp/c90.wuclients

###########################################################################
# Parameters for Tasks
###########################################################################

# Tasks are the different steps of an NFS factorization: polynomial
# selection, generating the factor base, sieving, etc.

# execpath: Specifies the search path for binary programs that should be run.
# Each binary program that is run can be specified with "execpath",
# "execsubdir", and "execbin" parameters; the script looks for the binary
# executable as "execpath/execsubdir/execbin" and "execpath/execbin", in this
# order. Defaults for these parameters are defined for each program in
# cadoprograms.py; usually it is sufficient to set "execpath".
#
# adjust if needed.
# tasks.execpath=$(builddir)

# verbose: Verbosity of program output. Many programs accept a -v (or
# similar) command line flag; this parameter specifies whether verbose
# output should be enabled. Has no effect on cadofactor itself, just on the
# sub-programs. Cadofactor's own output is controlled by the --screenlog
# command line parameter.
tasks.verbose = 1

# For tasks that use the client/server set-up, currently polynomial selection
# and sieving, set a time-out in seconds after which assigned workunits for
# which no result was returned get cancelled and re-submitted, so that other
# clients can (hopefully) finish them
tasks.wutimeout = 3600 # one hour

# After a total of maxtimedout workunits have timed out, the factorization
# is aborted, as such a large number of timed out workunits may indicate a
# problem with the clients. The default is 100. For large factorization on
# unreliable machines which can be stopped/restarted frequently, this limit
# is probably too small and should be increased.
tasks.maxtimedout = 100

# Various tasks refer to the factor base bounds lim0, lim1, and to the
# large prime bounds lpb0, lpb1. By adding these value to the root node of
# the parameter tree, all tasks can find them.

# (r,a) means rational or algebraic side
lim0 = 404327
lim1 = 811066
# The number of rational (resp. # algebraic) primes is roughly lim0/log(lim0)
# (resp. lim1/log(lim0))
lpb0 = 23
lpb1 = 23
# lpb0/lpb1 is the (base 2 log of the) large prime bound

###########################################################################
# Polynomial selection task with Kleinjung's algorithm (2008)
###########################################################################

tasks.polyselect.degree = 4		# degree of the algebraic polynomial

#tasks.polyselect.threads = 2
#    # How many threads to use per polyselect process

tasks.polyselect.P = 10000		# choose lc(g) with two prime factors in [P,2P]
    # Setting a large value will most likely find better polynomials,
    # but the number of found polynomials will be smaller.
    # As a rule of thumb, we want at least 100 found polynomials in total
    # (without norm limitation, see below).
    # We must have P larger than primes in the SPECIAL_Q[] list.

tasks.polyselect.admin = 0          # min value for leading coefficient of f(x)
    # If not set, the default is 0.

tasks.polyselect.admax = 150e3      # max value for leading coefficient of f(x)

tasks.polyselect.incr = 60	    # increment for leading coefficient of f(x)
    # This factor is usually a smooth number, which forces projective roots in
    # the algebraic polynomial. 60 is a good start, 210 is popular as well.
    # Warning: to ensure lc(f) is divisible by incr, admin should be divisible
    # by incr too.

    # The polynomial selection search time is proportional to the
    # length of the search interval, i.e., (admax-admin)/incr.

tasks.polyselect.nrkeep = 40

tasks.polyselect.adrange = 5000		# size of individual tasks
    # Polynomial selection is split into several individual tasks. The
    # complete range from admin to admax has to be covered for the polynomial
    # selection to complete. The number of individual tasks is
    # (polsel_admax-polsel_admin)/polsel_adrange. Each such task is issued as
    # a workunit to a slave for computation.

tasks.polyselect.nq = 256
    # Number of small primes in the leading coefficient of the linear polynomial
    # Safe to leave at the default value
    # Recommended values are powers of the degree, e.g., 625 for degree 5,
    # or 1296 for degree 6. Here 256 = 4^4 thus the leading coefficient of
    # the linear polynomial will be q1*q2*q3*q4*p1*p2 where q1,q2,q3,q4 are
    # small primes, and P <= p1, p2 < 2*P.

tasks.polyselect.sopteffort = 0
    # Indicates how much effort is spent in stage 1 (size optimization) of the
    # polynomial selection. The default value is 0. The size optimization time
    # is roughly proportional to 1+sopteffort.

tasks.polyselect.ropteffort = 0.2
    # Indicates how much effort is spent in stage 2 (rootsieve) of the
    # polynomial selection. The number of sublattices considered is
    # proportional to ropteffort, and the rootsieve time is roughly
    # proportional to ropteffort. This is a floating-point value.


###########################################################################
# Sieve task
###########################################################################

tasks.sieve.mfb0 = 46
tasks.sieve.mfb1 = 46
    # mfb0/mfb1 is the (base 2 log of the) limit for the cofactor we try
    # to split into large primes.

# tasks.sieve.lambda0 = 2.131250000000000
# tasks.sieve.lambda1 = 2.103125000000000
    # lambda0/lambda1 is the early-abort ratio: if after sieving, and
    # subtracting from the log of the norm the contribution of the
    # sieved primes, the approximation of the log of the remaining
    # cofactor is larger than lambda times lpb, we reject.
    # Note that this has in particular the effect that if e.g. lambda0 <
    # some integer k+1, then at most k large primes are allowed on the
    # rational side. It is customary, when at most k large primes are
    # allowed, to set lambda0 to e.g. k + 0.1, in order to compensate for
    # inaccuracies due to sieving.
    # In case lambda0 or lambda1 are not given (or have value 0),
    # a default value is computed from mfb0 or mfb1. Since the sieving
    # parameters are optimized without lambda0/lambda1, it is better to
    # leave them unspecified.

tasks.sieve.ncurves0 = 9
tasks.sieve.ncurves1 = 10
    # ncurves0 controls the number of P-1/P+1/ECM curves used in the
    # cofactorization on side 0 (i.e., rational side), more precisely we
    # use 3+ncurves0 curves. If one increases ncurves0, one will find more
    # relations, but the cofactorization cost will increase.
    # Same for ncurves1 on side 1 (algebraic side).
    # If unspecified, the number of curves is deduced from the value of lpb0
    # or lpb1, see function nb_curves() in file sieve/ecm/facul.c.


tasks.I = 11
    # Sieving range in lattice siever
    # The lattice siever sieves over a range in the (i,j) plane which is
    # 2^I times 2^(I-1), to put things simply (some rescaling may change
    # this, but the size of the sieve area remains constant). Increasing
    # I by 1 multiplies the amount of required RAM for the siever by a
    # factor of 4.
    # Note: now I is used also in the polynomial selection (for the computation
    # of Murphy's E value) thus we define it at the "tasks" level, not at the
    # "tasks.sieve" level.


tasks.sieve.sqside = 1
    # The sqside parameter specifies the side number where the special-q
    # is to be taken. sqside should be set on the side where the largest
    # norms are expected. By default, the polynomial selection puts the
    # algebraic polynomial on side 1, and that's where to put the
    # special-q.  For snfs numbers, sometimes it is better to put is on
    # the rational side.

tasks.sieve.qmin = 200923
    # Start of the special-q range
    # qmin is usually equal to the corresponding factor base bound, but this
    # is not a requirement - it can be both larger or smaller.

tasks.sieve.qrange = 10000		# The size of an elementary sieving task
    # The sieving process is split into many individual tasks, where each task is
    # assigned as a workunit to a client. This parameter controls the size of
    # individual tasks.

#tasks.sieve.threads = 2
    # The lattice siever program las may run in a multithreaded manner.
    # This keeps the amount of memory used constant, and just runs
    # faster.

###########################################################################
# Filtering task
###########################################################################

tasks.filter.purge.keep = 160		# minimal excess wanted after purge
tasks.filter.add_ratio = 0.1
    # Number of additional required relations before next filtering step
    # expressed as a ratio of the number of unique relations already collected
    # Default value is 0.1 (i.e. number of additional relation is equal to 10% of number of unique relations)
tasks.filter.maxlevel = 20		# perform up to <maxlevel>-way merges
tasks.filter.target_density = 170       # stop merging when the average density
				        # per row attains target_density
tasks.filter.required_excess=0.0        # Controls extra sieving:
                                        # completes the filtering only when
                                        # the relative excess (excess divided
                                        # by number of ideals) is larger than
                                        # the given value after all singletons
                                        # have been removed.
                                        # Default is required_excess=0.0 and
                                        # completes the factorization as soon
                                        # as a positive excess is obtained
                                        # (>= tasks.filter.purge.keep).

###########################################################################
# Linear algebra and characters tasks
###########################################################################

# - runtime environment -
# cado-nfs.py supports only running the linear system solving on the
## host running cado-nfs.py itself, using posix threads. More advanced
# usage has to go by hand.
#tasks.linalg.bwc.threads = 2x2		# Multithreading level of Block-Wiedemann ; Use
					# <mn> (a single integer) or <m>x<n>; if a single
					# integer is given, a split close to sqrt(mn) is chosen
tasks.linalg.bwc.interval = 100		# checkpointing interval for bwc.
tasks.linalg.bwc.interleaving = 0
tasks.linalg.m = 64
tasks.linalg.n = 64
tasks.linalg.characters.nchar = 50		# number of characters
#tasks.linalg.characters.threads = 2

###########################################################################
# Square root task
###########################################################################

## Number of threads used for the square root. Dependencies will be processed
# by blocks of 16 for example. If the number of dependencies is not a multiple
# of say 16, then the last block (if needed) will be reduced to the actual
# number of remaining dependencies.
## tasks.sqrt.threads = 16

# To start again at the first dependency (might be useful in debug mode):
# tasks.sqrt.first_dep = 0

###########################################################################
# Work unit server
###########################################################################

# address: The IP address where the server will listen for incoming
# connections from clients.  The default behavior if not set is to
# listen on all network devices (that is, to listen on the wild-card
# address 0.0.0.0).  If a hostname is given, will call gethostbyname()
# to discover the associated IP address and listen for connections
# only on that interface.  For example, listening on address
# "localhost" will only allow clients running on the same machine to
# connect.  You should not set this explicitly unless you really only
# want to listen on that particular address and interface.

# server.address = localhost

# port: The server of the client/server set-up will listen at this port
# If you want to run several cadofactor servers on the same machine, you
# need to specify a different port for each one.
server.port = 8001


# ssl: whether to use SSL for client/server communication.
# Without SSL authentication and encryption, it is possible for an attacker
# on the same network to modify workunits sent to clients and so effect
# execution of malicious code on the client machines. With SSL, clients
# are started with the server's certificate fingerprint as a command line
# argument, and verify the certificate when first connecting. The
# certificate is then used for all subsequent communication.
# The default is yes.

# server.ssl = yes


# whitelist: a comma separated list of IP ranges that should be allowed to
# contact the server. The IP ranges are given in CIDR format: an IP address,
# optionally followed by a slash and the number of most significant bits
# that need to match. For example, 192.168.0.0/24 means that all IP
# addresses in the range 192.168.0.0, ..., 192.168.0.255 are allowed.
# Here, hostnames can also be used in place of the IP address, their name
# will be resolved to an IP address.
# Note that the hostnames on which cadofactor starts clients, i.e., the
# hostnames in the slaves[...].hostnames parameters, are always added to the
# whitelist. You should have to whitelist addresses manually only if you
# want to start clients manually.

# server.whitelist = 192.168.0.0/24,localhost


###########################################################################
# Worker slaves
###########################################################################

# The cado-nfs.py script has the ability to spawn client scripts on many
# machines using ssh, in a master/slave manner. The client scripts then
# request workunits describing a small part of the computation that needs
# to be done, perform this computation, upload the result to the master
# running the cado-nfs.py script, then request and process the next
# workunit. Client scripts can be launched manually as well to participate
# in an ongoing factorization.

# Clients which cado-nfs.py should start are specified under "slaves".
# We look for any path under "slaves" with a "hostnames" key, then for each
# host name specified (comma separated list, with multiplicity), one slave is
# launched on that host.
# If the "hostnames" value is of the form "@filename", then the host names
# are read from the file "filename", one host name per line, with the same
# multiplicity rule.
# Note: if the host name for a client is "localhost", then the client is
# started directly as sub-processes of the cadofactor script; if it is not
# "localhost", then clients are started through SSH. This affects the
# client's current working directory: if the client is started as a
# sub-process, its CWD is the same as that of the cadofactor script; if it
# is started through SSH, its CWD is the CWD after an SSH login (usually the
# user's home directory).

# adjust if needed
# slaves.hostnames = localhost

# Example with multiple host names. This launches one client on localhost,
# two on otherhost1, and one on otherhost2, assuming there is no nrclients
# parameter (see below) that would override the multiplicity
# slaves.hostnames = localhost, otherhost1, otherhost1, otherhost2

# Modifies the behaviour of the "hostnames" list: if nrclients is specified,
# then multiplicity in the "hostnames" list is ignored, and "nrclients" client
# scripts are launched on each unique host name.
#
# adjust if needed
# slaves.nrclients = 2

# scriptpath: The path to the cado-nfs-client.py scripts, on the slave's file
# system.

# scriptpath must point to the directory in which the cado-nfs-client.py script
# can be found on the client machines (wuclient.py is also needed,
# although if it is found in $(sourcedir)/scripts/cadofactor, it is fine,
# too). I.e., with scriptpath as set in this example and if, for example,
# "sourcedir = /tmp/cado-nfs", then clients will
# be started through:
# "ssh [hostname] /tmp/cado-nfs/cado-nfs-client.py [client parameters]"
#
# adjust if needed
# slaves.scriptpath = $(sourcedir)

# downloadretry: Number of seconds a client should wait between failed
# download attempts from the server

# adjust if needed
# slaves.downloadretry = 10

# This sets the posix ``nice level'' for programs launched by a slave.
# If you don't know what it is, you may leave as is, or set to 0.

# adjust if needed
# slaves.niceness = 10		# nice level for the sieving jobs

# basepath: defines the base directory under which cado-nfs-client creates its
# download and working directories. It is permissible to let all clients of
# the same factorization use the same download directory; files are
# protected by locking to prevent download races. If the download directory
# is on a shared file system, then this filesystem must support locking.
# The working directories should be unique; the default choice for the
# working directory includes the clientid which should ensure uniqueness.

# Since this example runs its slaves on localhost, we use a subdirectory
# of the cado-nfs.py working directory, so that all files of both master
# and clients can be cleaned up easily.
#
# adjust if needed
# slaves.basepath = /tmp/c90.wuclients

# If you want to launch clients on multiple groups of slave machines, with
# different parameters per group, you can group them by different parameter
# paths, like so:
# slaves.downloadretry = 10
# slaves.basepath = /tmp/wuclient
# slaves.home.hostnames = localhost
# slaves.home.nrclients = 1
# slaves.home.scriptpath = $(sourcedir)/scripts/cadofactor
# slaves.work.desktop.hostnames = workstation1, workstation2
# slaves.work.desktop.scriptpath = /path/to/scripts/cadofactor
# slaves.work.cluster.hostnames = clusternode1, clusternode2, clusternode3
# slaves.work.cluster.scriptpath = /other/path/to/scripts/cadofactor
## Use 4 slaves on all "work" slaves, both "desktop" and "cluster"
# slaves.work.nrclients = 4

# Here, "home", "work", "desktop", and "cluster" are arbitrary alphanumeric
# strings; their sole purpose is to create different nodes in the parameter
# tree under which hosts that should use the same parameters can be grouped.
