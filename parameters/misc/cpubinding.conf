# This file is a helper file intended to teach bwc how to place its
# threads given the current machine processor topology. This
# functionality requires cado-nfs to be built with hwloc and C++11
# support.
#
# We provide here a list of families of processor topologies, for which
# we tell for each thread splitting how threads should be placed. This
# "manual" approach could conceivably be replaced by something automatic,
# but this is true only to a limited extent, since this involves choices.
# at some point.
#
# SOFTWARE REQUIREMENTS
# ---------------------
#
# The workhorse for understanding the machine topology as well for
# performing the CPU binding is the hwloc software package. Functionality
# here requires that hwloc be present. It is possible to point to a
# custom location for the hwloc software by setting the HWLOC variable in
# local.sh
#
# MACHINE TOPOLOGY
# ----------------
#
# Topologies are given in the form known as hwloc's "synthetic" format
# topologies. It is possible to obtain the description of a machine
# topology using the hwloc command "lstopo --of synthetic --no-io" (the
# bwc programs also print this topology). Some example topology strings
# are given below.
#
# barbecue   NUMANode:4 Socket:1 L3Cache:1 L2Cache:6 L1Cache:1 Core:1 PU:2 
# cassoulet  Socket:1 L3Cache:1 L2Cache:4 L1dCache:1 L1iCache:1 Core:1 PU:1 
# catrel     NUMANode:2 Socket:1 L3Cache:1 L2Cache:8 L1Cache:1 Core:1 PU:2 
# econome    NUMANode:2 Socket:1 L3Cache:1 L2Cache:8 L1Cache:1 Core:1 PU:1 
# frite      Socket:1 L3Cache:1 L2Cache:2 L1dCache:1 L1iCache:1 Core:1 PU:1 
# graphene   Socket:1 L3Cache:1 L2Cache:4 L1Cache:1 Core:1 PU:1 
# graphite   NUMANode:2 Socket:1 L3Cache:1 L2Cache:8 L1Cache:1 Core:1 PU:1 
# griffon    Socket:2 L2Cache:2 L1Cache:2 Core:1 PU:1 
# magret     Socket:1 L3Cache:1 L2Cache:4 L1dCache:1 L1iCache:1 Core:1 PU:1 
# paradent   Socket:2 L2Cache:2 L1Cache:2 Core:1 PU:1 
# parapluie  Socket:2 NUMANode:2 L3Cache:1 L2Cache:6 L1Cache:1 Core:1 PU:1 
#
# NOTE: as of hwloc-1.11, the term "Socket" has been renamed to
# "Package". cado-nfs recognizes both terms interchangeably.
#
# FILE STRUCTURE
# --------------
#
# The present file is split into sections in ini-file style, and only the
# sections which match the machine topology are considered, with the
# following rules:
#  - a section with exact match is always preferred, and wins over all
#  other possibilities.
#  - for brevity, abbreviated matching rules are defined. Only one
#  abbreviated rule is allowed to match (see ABBREVIATED MATCHING RULES
#  below).
#
# Each section then contains several lines of the form
# "thr=<integer>x<integer> <mapping>", which indicates how the
# corresponding thr= command-line argument must be understood in terms of
# cpu binding. See THREAD MAPPING SYNTAX below.
#
# ABBREVIATED MATCHING RULES.
# ---------------------------
#
# A section title is expected to contain a topology string in the format
# mentioned above, which is a space-separated list of
# <object_type>:<arity> pairs. Alternatively, as an abbreviation help,
# the section title may contain special tokens of the form "@<keyword>".
# Those mean that in order to compare the machine topology M with the
# section title S, a substitution phi(M) is first computed, and the
# result is compared to S' which has the special token taken out. The
# following substitutions are recognized.
#
# @merge_caches: all levels which match the regexp L[0-9]Cache[id]? are
# merged with the level below, which has to be "Core".
#
# @group_PU: takes all PU objects sharing everything above (thus, most
# likely, hyperthreaded logical cores) as a single scheduling slot. This
# implies that PU*(number)=>1x1 is implicitly appended to the mapping
# string unless another PU rule is already present (see THREAD MAPPING
# SYNTAX below).
#
# It is possible to print the abbreviated topology descrption which would
# map the current machine's topology with the following code:
#
# lstopo --of synthetic --no-io | perl -pe 's/PU:\d+/\@group_PU/; s/\(\w+=[\d*:]+\)//g; do {} while s/\b\w+Cache[a-z]?:(\d+) Core:(\d+)/"Core:" .$1*$2 ."m"/e; s/(Core:\d+)m+/\@merge_caches $1/;'

# which should gives abbreviated descriptions such as:
# barbecue   NUMANode:4 Socket:1 @merge_caches Core:6 @group_PU
# cassoulet  Socket:1 @merge_caches Core:4 @group_PU
# catrel     NUMANode:2 Socket:1 @merge_caches Core:8 @group_PU  
# econome    NUMANode:2 Socket:1 @merge_caches Core:8 @group_PU 
# frite      Socket:1 @merge_caches Core:2 @group_PU
# graphene   Socket:1 @merge_caches Core:4 @group_PU 
# graphite   NUMANode:2 Socket:1 @merge_caches Core:8 @group_PU
# griffon    Socket:2 @merge_caches Core:4 @group_PU 
# magret     Socket:1 @merge_caches Core:4 @group_PU
# paradent   Socket:2 @merge_caches Core:4 @group_PU 
# parapluie  Socket:2 NUMANode:2 @merge_caches Core:6 @group_PU
#
# Note that alternatively, the bwc programs also print this abbreviated string.
#
# THREAD MAPPING SYNTAX
# ---------------------
#
# a thr= command-line argument specifies a 2d mesh with strict
# dimensions. The mapping must indicate how this 2d mesh is to be split
# into pieces which keep a relative contiguity, according to the machine
# description.
#
# The pair of dimensions given by the thr= command-line argument are
# denoted by n0 and n1 below.
#
# The thread mapping is a space-separated list of strings whose format
# matches the following perl regexp:
#       (\w+)(\*\d+)?=>(\d+)x(\d+) .
# Strings are interpreted in reverse order, from the last one to the
# first one.  For each string, suppose the different () groups in the
# regexp are denoted such that the string looks like
#       $type*$group=>${t0}x${t1} ,
# where $group may be undefined (there is no * then).
#
# Roughly put, the string says that the thread mesh but be split in
# chunks of size t0*t1, and those must be pinned to objects of type
# $type, whose number may be explicitly stated as $group.
#
# Since processing happens iteratively, the view becomes coarser and
# coarser. Once the thread mesh has been split in chunks, we have a mesh
# of chunks, which we call "thread groups". Likewise, since each step
# defines pinning to objects of a certain types, the way we consider the
# topology must be modified as processing goes, with cpu cores, for
# examples, becoming groups of cpu cores.
#
# The following rules apply when processing strings of thread mapping
# descriptions.
#
#  - $type must be one of the hwloc object types appearing in the section title.
#    In particular, if @merge_caches has been used, cache levels may not
#    be used as indicators of splitting information.
#  - strings must mention the hwloc object types exactly in the same
#    order as they appear in the section title, although not all section
#    title objects need to be mentioned. A given section title object
#    type may be specified more than once, see below for an example.
#  - t0 and t1 must divide n0 and n1, respectively. For the next
#    processing step, the same rule applies with n0 and n1 replaced by
#    n0/t0 and n1/t1. This means that a sub-mesh of the thread mesh will
#    be pinned (either collectively or individually or on a
#    thread-by-thread basis, see below) to a set of objects. Hereby, we
#    establish a mapping "group of threads" => "group of cores", and both
#    are to be viewed as single items for further processing.
#  - presence or absence of $group determines two distinct situations.
#    - if $group is undefined, then pinning is on a thread-by-thread
#      basis (or thread-group by thread-group, more accurately). A subset
#      of t0*t1 among the objects of type $type are reserved for pinning
#      the t0*t1 threads. IN THIS CASE, t0*t1 MUST DIVIDE THE NUMBER OF
#      $type OBJECTS (or, more accurately, of "groups of $type" which
#      remain).
#    - if $group is specified, then mapping is done collectively: a group
#      of $t0*$t1 threads will be allowed to freely migrate within a set
#      of $group objects of type $type. IN THIS CASE, $group MUST DIVIDE
#      THE NUMBER OF $type OBJECTS. Note that $group may be strictly
#      greater than ${t0}*${t1} (when we have spare resources), stricty
#      lower than ${t0}*${t1} (then the machine is over-subscribed), or
#      even equal. If we have $group==${t0}*${t1}, then this is not the
#      same as having $group undefined, as $group==${t0}*${t1} indicates
#      that migration is allowed.
#    - $group may ONLY be defined for the very first string(s) processed,
#      that is for any (arbitrary length) terminating segment of the
#      mapping description.
#  - in the end, the product of all t0's must equal the initial n0, and
#    likewise for t1 and n1.
#
# Here are a few example thread mapping fragments.
#
# PU*2=>1x1
#       This string is important. It means that the bottommost object
#       type of the hwloc description, which corresponds to hyperthreaded
#       cores, should correspond to single scheduling units, allowed for
#       only *one single thread*. This is automatically added by the
#       @group_PU syntax.  Note that if enabled (by default it's not)
#       bwc's interleaving mechanism would however inevitably bind two
#       threads to the same hyperthreading pair.
#
# Core=>2x1 Core*4=>1x3
#       Imagine we consider a Core:8 topology. We want to schedule 3
#       threads on a group of 4 cores, and then identify two distinct
#       group of 3 threads, to be pinned to two distinct groups of 4
#       cores. We thus get:
#       threads (0,0) to (0,2) -> pinned to cores 0,1,2,3
#       threads (1,0) to (1,2) -> pinned to cores 4,5,6,7
#
# Note that an empty mapping string is a legitimate way of saying that we
# want all processor binding to be undone.
#
# Alternatively, it is possible to specify "remove" as a CPU binding,
# which undoes any preexisting binding.
#
# FILLING THE GAPS
# ----------------
#
# This file can't be complete, by definition. Nor do its contents pretend
# to provide universally good solutions. It's just handy to have a unique
# place to remember the settings used, based on experience. It is thus
# expected that data below goes with some comments to indicate the "why
# and how".
#
# If you want to experiment with pinning and there's no info here, and
# you don't want to read my complete rant, here's a hint. Assuming you're
# non-NUMA, and you have a single socket with a number of cores which
# corresponds precisely to the number of threads you want (say, 10, and
# you want thr=2x5): pass --cpubinding="Core=>2x5".

[Socket:2 NUMANode:2 @merge_caches Core:6 @group_PU]
# examples: parapluie

[Socket:2 @merge_caches Core:4 @group_PU]
# identical to [Package:2 @merge_caches Core:4 @group_PU]
# examples: griffon, paradent
thr=2x4 Package=>1x2 Core=>2x2

[NUMANode:4 Socket:1 @merge_caches Core:6 @group_PU]
# examples: barbecue
thr=2x3 Core=>2x3
thr=2x6 NUMANode=>1x2 Core=>2x3
thr=4x3 NUMANode=>2x1 Core=>2x3
thr=8x6 NUMANode=>2x2 Core=>2x3 PU*2=>2x1
thr=6x8 NUMANode=>2x2 Core=>3x2 PU*2=>1x2

[NUMANode:2 Socket:1 @merge_caches Core:8 @group_PU]
# examples: graphite
# examples: catrel, econome
# examples: parasilo
thr=2x4 Core=>2x4
thr=4x2 Core=>4x2
thr=4x4 NUMANode=>1x2 Core=>4x2
# fictitious (as we probably don't want to do that for real):
thr=5x3 NUMANode=>1x3 Core*8=>5x1
thr=2x3 Core=>2x1 Core*4=>1x3
thr=1x2 Core=>1x2
# thr=8x4 NUMANode=>1x2 Core*8=>8x2
# thr=8x4 NUMANode=>1x2 Core=>4x2 PU=>2x1
# thr=8x4 NUMANode=>1x2 Core=>2x2 Core=>2x1 PU=>2x1
thr=8x4 NUMANode=>1x2 Core=>4x1 Core*2=>2x2
thr=16x2 NUMANode=>1x2 Core*8=>16x1
thr=2x16 NUMANode=>2x1 Core*8=>1x16

[Socket:1 @merge_caches Core:4 @group_PU]
# examples: cassoulet, magret, graphene
thr=2x3 Core=>2x1 Core*1=>1x3
thr=2x2 Core=>2x2
thr=3x3 remove


[Socket:1 @merge_caches Core:2 @group_PU]
# examples: frite
thr=2x1 Core=>2x1
thr=2x2 Core=>2x1 Core*1=>1x2
thr=2x3 Core=>2x1 Core*1=>1x3

[NUMANode:4 Socket:1 @merge_caches Core:12 @group_PU]
# examples: raminator
thr=6x8 NUMANode=>2x2 Core=>3x4


[Socket:4 @merge_caches Core:6 @group_PU]
# examples: lattice0
thr=4x6 Socket=>2x2 Core=>2x3

[NUMANode:2 Package:1 @merge_caches Core:14 @group_PU]
thr=4x7 NUMANode=>2x1 Core=>2x7

[NUMANode:4 Socket:1 @merge_caches Core:14 @group_PU]
# example: wurst
thr=8x6 NUMANode=>2x2 Core*14=>4x3
thr=8x7 NUMANode=>4x1 Core=>2x7
thr=6x4 NUMANode=>2x2 Core*14=>3x2

[NUMANode:2 Package:1 @merge_caches Core:12 @group_PU]
# siv-1
thr=6x4 NUMANode=>2x1 Core=>3x4
thr=4x6 NUMANode=>1x2 Core=>4x3

[NUMANode:2 Package:1 @merge_caches Core:22 @group_PU]
# siv-2
thr=5x8 NUMANode=>1x2 Core*22=>5x4
thr=8x5 NUMANode=>2x1 Core*22=>4x5

[NUMANode:2 Package:1 @merge_caches Core:18 @group_PU]
# lattice1
thr=6x6 NUMANode=>2x1 Core=>3x6


