
*** THIS DOCUMENTATION IS NOT REGULARILY CHECKED TO BE UP-TO-DATE. ***

20160905 continuation of the previous update. mf_bal and dispatch now
         second-class citizens.
20160820 scattered update ; removed some deprecated stuff.
20160201 edit the MPI section
20130724 general update

See bwc-ptrace.sh and bwc-ptrace.m for the most up-to-date checks
relative to the exact meaning of most files for the case of linear
systems over GF(p) or GF(2). More generally, all GF(p) specific
information is in README.gfp


The tools present in this subdirectory form an implementation of the
Block-Wiedemann algorithm with the following features:

- parallelism at a multithread / MPI level. No fully integrated
  multi-site level yet, but bare bones are there.
- checkpointing / restarting in case of problem.
- checksums during the computation to detect problems.
- possibility to compute either a left or right kernel vector.
- cpu binding (if hwloc is available)

Properly documenting bwc implies a bit of knowledge of the block Wiedemann
algorithm and its parameters. This README does not cover this aspect.

Input, output, and formats.
===========================

The input is a matrix that comes out from merge (the .small matrix). Two
input formats are supported.
 - The legacy ascii format uses one header line saying nrows,
   ncols, plus <nrows> rows, with <nentries> [<column indices of non-zero
   coefficients>].
 - The binary format is headerless. A file [PREFIX].bin is expected to
   contain nrows rows, each coded as the row length <nentries> written as
   a 32-bit little-endian integer, followed by the <nentries> indices of
   non-zero columns, all as 32-bit little-endian integers. Companion
   files named [PREFIX].rw.bin and [PREFIX].cw.bin can be present or
   generated by the tool called mf_scan. These indicates the row weights
   and column weights, always as 32-bit LE integers.

The output is a set of vectors of the matrix nullspace. Either left
nullspace (zero combination of rows) or right nullspace (zero combination
of columns) may be computed. The kernel ends up in the file called W in
the chosen working directory.

Split width
===========

A quantity which is pervasive in the code is the "split width". This
really means 64 when we're talking binary linear systems, and 1
otherwise. The block Wiedemann sequences are defined for blocks of
vectors whose width is exactly this "split width", understood as
sub-blocks of the big vector blocks with n columns.

Some of this documentation was written specifically with the binary case
in mind, whence the appearance of "64" below should really be understood
as "split width".

Calling sequence of binaries
============================

We describe here the list of binaries to run. This can be somewhat automated
using the bwc.pl driver script, documented below.

 1 - prep
     select input blocks of vectors, and check that rank conditions are
     satisfied
 2 - secure
     precompute some data useful for checksums
!3 - krylov
     compute the Krylov sequence
 4 - acollect
     collect the Krylov sequence files into one big file. Can be done at
     several moments in the computation.
!5 - lingen
     Berlekamp-Massey -like step: produces a generating polynomial
!6 - mksol
     create the solutions from the polynomial
 7 - gather
     reconstruct solutions from the mess

krylov, lingen, and to a lesser extent mksol are the steps having a real computational impact.
prep is cheap but important for ensuring the success of the algorithm.
acollect and gather are just trivial bookkeeping.
secure is akin to doing a few krylov iterations. It runs for roughly just
as long as the desired time between two recoverable checkpoints (the
longer it runs, the less often checks are done -- but checks are
relatively cheap anyway). It is possible to skip this step using the
skip_online_checks option (see below).

Parameters
==========

All binaries accept command-line parameters. As per the CADO params.[ch]
conventions, several syntaxes are possible, but the current documentation
suggests only the <key>=<value> form.

Note that some of the core operations performed by binaries may depend on
the command-line specified parameters. The CADO code embarks pre-computed
code for common sets of parametsrs. Working with blocks of vectors of
width 64 utilizes code which has inlined functions specific to this
width. Programs achieve versatility w.r.t. this width by loading the
corresponding code from shared libraries. These shared libraries, named
e.g. libmatmul_u64k1_bucket.so for the default compiled library, are
expected to be present in the same directory as the calling binary.

* wdir=<directory name>

    Work in the specified directory. This implies that the program will cd
    into that directory prior to doing any disk access, so relative path might
    trigger bugs. All output will appear in the specified directory.

    Having a wdir is generally a good idea, as bwc will otherwise happily
    populate your working directory with data files.

* matrix=<path>
* balancing=<path>
    path to matrix file, and to balancing file. May be absolute paths,
    otherwise understood as relative from the current directory (working
    directory). Note that the matrix cache files are named relative to
    that directory as well (unless the local_cache_copy_dir parameter is
    used). The ``matrix'' parameter may point to a remote URI in any
    format recognized by the cURL library, if the use of the latter has
    been enabled.

    A balancing file can be created by the "mf_bal" binary. The
    thread-specific matrix cache files can be computed with the
    "dispatch" binary. None of these calls is necessary, as the task is
    performed automatically if defaults parameters are deemed fine.
    
* m=<number>, n=<number>

    Set the corresponding parameters from the block Wiedemann algorithm. Note
    that these are the _global_ values. Presumably both are multiples of 64.
    If two sites are meant to compute two sequences independently, the n value
    here is the sum of the sequence widths.

* nullspace=[left|right]

    Look for left or right kernel vectors. If one looks for a right kernel
    vector, then the matrix-times-vector operation will be performed.
    Otherwise, the vector-times-matrix operation is used, or equivalently
    transpose-of-matrix times vector.

* mpi=<number1>x<number2>

    Work on a grid of <number1>x<number2> nodes. <number1> rows, <number2>
    columns. The ``rows'' and ``columns'' receive matrix blocks according to
    their positioning.

* thr=<number1>x<number2>

    Same on the thread level. This is not exclusive of the above.

* interleaving=[0|1]

    Whether to compute _two_ sequences simultaneously. This is relevant when a
    significant amount of time is spent in matrix communications, so that
    while communication is taking place, the CPU is idle and can be put at
    work. This doubles the number of threads of the program, and increases the
    memory footprint a bit (the matrix data which can be shared is indeed
    shared).

* interval=<number>

    Check for consistency every <number> matrix product iterations. Note
    that this also governs the number of intermediary vectors which are
    stored. If the value here is too small, then there are chances that
    you exhaust your disk space.

* rhs=<file>

    Indicate that we want to solve an inhomogeneous linear system. Right
    now this only works over GF(p), for a system of the kind M*X=R*Y (so,
    "nullspace=right"), and with n being at least as large as the number
    of columns of R.

* keep_rolling_checkpoints=<n>
    instructs the program to discard the rolling vector checkpoints every
    <n> checkpoints. This assumes for example that checkpoints are
    offloaded to external storage in the background by another process.
    Use of this option is conjunction with skip_online_checks=1 is
    dangerous, since it leaves open the possibility of a failure which
    stands no chance to be eventually detected.

* checkpoint_precious=<K>
    overrides the behaviour of keep_rolling_checkpoints and keeps all
    checkpoints which correspond to an iteration count which is a
    multiple of K.

The following parameters are also passed by bwc.pl to the relevant
sub-steps

* start=<value>, end=<value>

    limits for doing partial krylov/mksol

* ys=<number y0>..<number y1> (note the ".." separator)

    specify which sub-sequence to work with.  In normal usage y0 and y1
    should be consecutive multiples of the split width.  When
    interleaving=1, the interval y0..y1 is divided into two sub-intervals
    describing two subsequences whose computation is interleaved. The
    quantity K=(y1-y0)/(1+interleaving) quantifies the operating width of
    the innermost arithmetical operations ; when K=64, operations are
    based on unsigned longs (on 64-bit machines). Usual values for K are
    64 and 128.  Note that when ys is different from 0..n, then
    computation only involves some of the needed subsequences, not all --
    and all subsequences must be computed.

* solutions=<s0>-<s1> (note the "-" separator -- the divergence with the
  ".." separator above ought to be fixed someday).

    This is relevant to mksol and gather. This indicates
    (unsurprisingly), out of the n possible solutions which can be formed
    by the block Wiedemann algorithm, the ones we choose to produce. As
    above, it is expected that s0 and a1 are consecutive multiples of the
    split width, so that mksol can do matrix times vector operations on
    data whose size is the split width.

Some more internal parameters.

* mm_impl=[basic|sliced|bucket|basicp|zone]
    low-level matrix multiplication code to use. Several such back-ends
    are programmed. The first three above are for systems defined over
    GF(2), while "basicp" and "zone" are for systems over prime fields.
    As the name suggests, the two "basic" variants are intentionally
    simple and stupid. In contrast, the "bucket" and "zone"
    implementations are the faster ones, and should be used for better
    performance (the implementation defaults to using these, depending on
    which value is chosen for the prime).

* seed=<number>

    random seed. the ``prep'' program is not deterministic, so for debugging
    purposes, setting this parameter to a fixed value may be wise.

* lingen_threshold=<number>

    Threshold for recursive algorithm, only relevant for lingen. <number> is
    currently 64.

* Some cpu-level parameters, relevant for the low-level routines.
    - l1_cache_size=<number>
    - l2_cache_size=<number>
    - cache_line_size=<number>
        The length of some data blocks is set in accordance with these values.
        You may benefit from tuning them, but presumably only after you've
        gained some familiarity with the relevant source code.
* mm_* : parameters passed on to low-level matrix multiplication routines.
    - mm_store_transposed=[0|1] forces whether the matrix is stored row-major
      (0) or column-major (1). The low-level implementations are written with
      a preferred ordering in mind which is set automatically, so don't change
      this unless you're willing to try out with something new.
    - other mm_xxx_* parameters are not yet documented.
* sequential_cache_build=[0|1] whether the in-memory caches must be built
  one after another (=1), or all in parallel (=0, default behaviour).
  Since this operation requires significantly more RAM than what is used
  within the actual computation, it makes sense.
* skip_online_checks=[0|1] whether krylov and mksol should check their
  vector multiples against the check data computed by the ``secure''
  step. The default is to do the check. If it is not done, it must be
  done externally (by a program which would be trivial to write but which
  incidetally has never been written).
* save_submatrices=[0|1] save the sub-matrices used by each node in
  simple binary files. These files are not used by the computation, so
  this option is not enabled by default.


Files (standard name, standard place, etc)
==========================================

This list links at who creates what and uses what. The prefix ``mat'' is
actually replaced by the basename of the input matrix, with suffixes
removed.

wdir/mat.<nh>x<nv>.bin  permutations used for balancing.
wdir/mat.<nh>x<nv>.<bchecksum>.bin  permutations used for balancing.
    Both files are identical. The former is used as a shorthand.
    Files are created by either prep, secure, krylov, or mksol the first time a
    balancing of total size <nh>x<nv> is to be used. It is also possible
    to use the standalone binary mf_bal for that.
    These files are read by prep, secure, krylov, or mksol.
wdir/mat.<nh>x<nv>.hi.vj.bin        sub-matrix files.
    created by prep, secure, krylov, or mksol the first time a
    balancing of total size <nh>x<nv> is to be used, and if and only
    if the save_submatrices option is set (useful for debugging)
wdir/mat.<nh>x<nv>.<bchecksum>.hi.vj-MMMMMM.bin       in-memory version of the sub-matrices
    created by {prep,secure,krylov,mksol,gather} if not found.
    used by {prep,secure,krylov,mksol,gather}
wdir/X                              X vector used for Krylov.
    created by prep
    read by krylov and secure
wdir/C.<n>                          check vector = trsp(M)^n * X  (See note (T))
    created by secure
    read by krylov
wdir/V<n1>-<n2>.0                   columns <n1>..<n2> (n2 exclusive) of Y
    created by split ifile=Y.0 ofile-fmt=V%u-%u.0 splits=[...,]<n1>,<n2>[,...]
    read by krylov mksol
wdir/A<n1>-<n2>.<j1>-<j2>       krylov sequence data.
    created by krylov
    read by acollect. acollect replaces all these files by a file matching the
    same pattern, read in turn by lingen.
wdir/V<n1>-<n2>.<j>             columns <n1>..<n2> of M^j Y (See note (T))
    created by krylov while computing.
    never read again unless for external checking (not all of it implemented
    yet)
wdir/F.sols<s1>-<s2>.<j1>-<j2>  linear generator.
wdir/F.sols<s1>-<s2>.<j1>-<j2>.rhs  linear generator.
    created by lingen
    read by mksol
    the .rhs files are only defined for j2 <= the total number of rhs
    vecgtors.
wdir/S.sols<s1>-<s2>.<n1>-<n2>  mksol sequence data.
    created by mksol
    read by gather
wdir/W                          kernel vectors
wdir/W.sols<s1>-<s2>.<j>        kernel vectors
wdir/K.sols<s1>-<s2>.<j>        kernel vectors
wdir/K.sols<s1>-<s2>.<j>.txt    kernel vectors
    created by gather

The permutation P
=================

bwc uses a shuffled matrix times vector product, which as a side
effect of the matrix times vector product also multiplies by a certain
permutation matrix. This is done for best efficiency.

This permutation matrix is as follows. We assume that the matrix m is
split in nh horizontal and nv vertical strips. M is first and foremost
padded trivially to obtain a larger matrix Mx whose number of rows and
columns are equal, and both multiples of nh*nv, say we have
nr==nc==nh*nv*nz for an integer nz.

Each row index i in [0..nr] can be written (i*nv+j)*nz+k, 0<=i<nh,
0<=j<nv, 0<=k<nz. The image by P of this index is (j*nh+i)*nz+k (or the
opposite, see bwc_trace.m to see which is right). Magma code for
computing the corresponding permutation matrix follows:

    pr:=func<x|(qr*nh+qq)*nz+r+1 where qq,qr is Quotrem(q, nv) where q,r is
    Quotrem(x-1,nz)>;
    Pr:=PermutationMatrix(GF(2),[pr(x):x in [1..nrx]]);


Per-program documentation
=========================

For all programs, more extensive documentation can often be found in the
source file comments if there are such comments.

mf_bal
------
    (this step gets run automatically behind the scenes the first time
    one runs one of the prep, secure, krylov, mksol, or gather binaries)

    mf_bal computes a row and column permutation which is applicable to
    the input matrix M, to be transformed into a twisted matrix
    Sr*M*Sc^-1 which such that when split in a 2d rectangular grid
    matching the job organization, one expect the blocks to be reasonably
    balanced in terms of number of coefficients.

    Sc is the permutation sending 0 to the first 32-bit integer found
    in col_perm, etc.  The current code enforces Sr == Sc.

    mf_bal does not actually read the input matrix, but works only with
    the row and column frequencies.

    mf_bal is not an mpi program.

    mf_bal computes a ``bchecksum'' which identifies the permutation
    being used.

    Note that as of 20110321, output files of bwc are invariant under
    this permutation. Even though internal computation uses it, this is
    transparent to the user.

dispatch
--------
    (this step gets run automatically behind the scenes the first time
    one runs one of the prep, secure, krylov, mksol, or gather binaries)

    dispatch uses the mf_bal output to compute the per-job in-memory
    data.  mat.<nh>x<nv>.hi.vj denotes the (i,j) block of the
    matrix, but is normally not saved to disk. Rather, the copies
    mat.<nh>x<nv>.<bchecksum>.hi.vj-XXXXXXXX.bin of the in-memory
    structures are saved.

    <ugly>
        There is an nfs-related kludge here, which may make bwc
        inappropriate for foreign matrices (this is fixable). We rely on
        the fact that the weight of the *columns* of the input matrix
        (assuming it's stored on disk row by row) is most unbalanced, so
        that mf_bal has computed Sc first, and Sr hardly matters. We
        expect Sr==Sc. Write Mt the matrix corresponding to the local
        blocks as they are present on the nodes/cores. The ``shuffled
        product'' will actually mutiply by Mt*Pr^-1 (when multiplying on
        the left), or P^-1*Mt when multiplying on the right.

        For this reason, the blocks dispatched are not the blocks of the
        matrix Sr*M*Sc^-1, but those of the matrix Mt==P*Sr*M*Sc^-1. This
        way, we are working with iterates of the matrix P*Sc*M*(P*Sr)^-1
        for nullspace=left, and Sr*M*Sc^-1 for nullspace=right. For
        Sc==Sr, these are conjugates of M in both cases.
    </ugly>

    All vector files considered *internally* later on, and relative to
    the matrix Mt, are ``twisted'' by the permutation related to the one
    computed by mf_bal. Output vectors are not affected, they are
    invariant under the permutation. For reference, here is how the
    internal data looks like (bwc-ptrace.m is supposed to have
    authoritative check lines for this).

    In the case of nullspace=left, we have
        Ytwisted = Y * (P*Sc)^-1 -- or P*Sc*Y if written transposed.
        Ytwisted * Mt = Y * M * (P*Sr)^-1 = Y * M * Sr^-1*Sc * (P*Sc)^-1
    and for nullspace=right:
        Ytwisted = Sc * Y
        Mt * Ytwisted = Sr*M*Y = Sc * Sc^-1*Sr * M * Y.
    Thus in the case where Sc==Sr, it is possible to untwist the twisted
    files in a consistent way.

build
-----

    This program does not belong to the usual sequence of binaries. Yet,
    it is a simple program whose job is only to create the in-memory
    cache from the matrix blocks mat.hi.vj (which get created if and only
    if the option save_submatrices=1 is used); note in particular that
    creating this in-memory cache is a memory-intensive operations, which
    typically requires two to four times as much memory as the program
    will eventually use once the matrix has been treated.

    The syntax for build is:

        build <ascii file> <impl> [left|right]

    E.g.:
        build /tmp/wdir/mat.h0.v0 bucket left

    Note that the [left|right] parameter must match the nullspace parameter
    used globally for bwc.

    Currently this program computed cache for width 64 bits, but this can
    be changed trivially to another size by changing the source, or
    supporting an extra parameter.  Note also that choosing the data
    width has an impact on the cached matrix file which is created. It
    must match the width which is used eventually for krylov/mksol,
    otherwise data block sizes might be chosen too large or too small. In
    the latter case, you will encounter the warning ``cached matrix file
    fits data with different striding''. This is not innocuous for
    krylov/mksol, as it hurts performance.

    Once /tmp/wdir/mat.h0.v0.bucket.bin (or /tmp/wdir/mat.h0.v0.bucketT.bin)
    are created, /tmp/wdir/mat.h0.v0 is normally never read again.

prep
----

    prep looks for sensible starting vectors X and Y for use with the block
    Wiedemann algorithm. X is a block of m vectors, Y a block of n vectors.
    prep ensures that the rank conditions are satisfied for the first few
    iterates trsp(X)*M*Y, trsp(X)*M^2*Y, trsp(X)*M^3*Y (See note (T)).

    En route, prep often creates the in-memory cached matrix file, unless the
    build program mentioned above has been called already. If several
    independent sequences are to be computed, this is a problem, as prep will
    build a cache for n-bit wide vector entries, while krylov/mksol will in
    this case work with shorter sequences.

    X and Y are stored as:

    X                             X vector used for Krylov.
    V<n1>-<n2>.0                  V vector used for Krylov.

    The V files correspond together to columns n1 to n2 of Y. n1 and n2
    are two consecutive multiples of the split width.



secure
------

    secure reads the output from prep, and creates:
    
    C.<interval>         check vector = trsp(M)^<interval> * X
    
    When M^(k*interval)*Y has been computed, using C one can look ahead the
    value trsp(X) * M^((k+1)*interval) * Y which will be computed after
    computing M^((k+1)*interval)*Y. This helps making sure that no bit flipped
    in the computation.

    Note that this checking mechanism is incomplete as it does not provide
    off-line checking. Off-line checking is possible at moderate cost, and
    will be implemented soon.

    (See note (T) for the whole paragraphs above).

krylov
------

    krylov produces the linear sequence (a_ij), as a set of files named:

    A<n1>-<n2>.<j1>-<j2>

    Such a file contains j2-j1 consecutive blocks of m by (n2-n1) bit
    matrices, stored in binary format, row major. 

    n1 and n2 are constant throughout the krylov run, and are specified
    with the ys=<n1>,<n2> command-line argument.

    n2-n1 determines which shared library the binary loads for performing
    its lowest-level arithmetic operations. For n2-n1==64, the shared
    library named libmatmul_u64k1_MMMMMMM.bin is loaded.
    
    The j-j1-th matrix in A<n1>-<n2>.<j1>-<j2> (for j1<=j<j2) is equal to
    trsp(X)*M^j*Y (see note (T)).

    krylov also creates
    
    V<n1>-<n2>.<j>      columns <n1>..<n2> of M^j Y. (see note (T)).

    This can be used to resume computations at iteration j.

    (j2-j1 is always equal to <interval> as in C.<interval>, and j is
    always a multiple of <interval>).

    When V<n1>-<n2>.<j2> is computed, krylov verifies that it is in
    accordance with the input vector V<n1>-<n2>.<j1>. The file
    A<n1>-<n2>.<j1>-<j2> is written only if this test succeeds. Otherwise the
    program aborts.

    Note on resuming. If the extra parameter start=<j> is passed to krylov,
    then computations resume after iteration <j>. Instead of
    V.<n1>-<n2>.0, V.<n1>-<n2>.<j> is read instead, and
    only subsequent A files will be touched. It is also possible to change the
    end value for the computation, in order to stop the subsequence
    computation prematurely.

    Note on interleaving. Selecting interleaving is functionally
    equivalent to running two instances of krylov, one with
    ys=<j1>..<jmiddle>, the other one with ys=<jmiddle>..<j2>. So
    with ys=0..128, the data width used by each instance is 64.

acollect
--------

    acollect does a trivial bookkeeping task. Consider the files
    A<n1>-<n2>.<j1>-<j2> as representing a rectangle in the space [0,<n>[ *
    [0,jmax[. acollect verifies that these rectangles do not overlap, and
    creates a unique file A0-<n>.0-<jmax>

    If the --remove-old argument is given, the newly created file replaces the
    old files.

lingen
------

    lingen computes the linear generator F from the sequence A. It
    represents an n times n matrix of polynomials of degree ceil(N/n)
    which satisfies

    A * F = matrix of polynomials of degree < ceil(N/n)

    F is stored coefficient by coefficient. Each coefficient is an n
    times n bit matrix, stored row-major.

    Note that lingen is currently the oldest code in bwc. Some parts of
    it were written when the corresponding algorithm barely existed. It
    admittedly deserves a rewrite.

    lingen is called by default with the --split-output-file option,
    whence the generator is saved as (n/s)^2 submatrices of size s*s,
    where s is the "split width" (64 for p=2, 1 otherwise).

mksol
-----

    This reads the files F.sols<s1>..<s2>.<n1>..<n2> as well as
    V<n1>..<n2>.<start> (for start=0, but really any multiple too), and
    creates files:

    S.sols<s1>-<s2>.<n1>..<n2>      partial sums for evaluation.

    All the S* files must be summed to form the final kernel vector
    element.

    note: off-line checking for mksol is not implemented yet.  mksol does
    checks in the same way krylov does, and saves over the V* files as
    well, which is admittedly silly since these files are likely to exist
    already, or will never be reused anyway except for off-line checking
    (which, as said, is not implemented yet).

gather
------

    This sums all S* files, and check how many times one must iterate in
    order to obtain M^k*(sum_of_S) == 0 (see note T). Then it stores:

    W                   M^(k-1)*sum_of_S (see note T).

Using bwc.pl
============

    bwc.pl is a perl script which tries to be intelligent, and runs the
    programs above in the correct order. It does cope with multi-node MPI
    environment, but does not yet handle several independent sequences
    (except for interleaved jobs).

    The spirit is that all parameters listed above are to be given to
    bwc.pl ; the _first_ provided argument must be either the basename of
    a specific binary to call (with possibly the proper mpiexec arguments
    added, and possibly the unrecognized arguments stripped out), or
    the meta-command :complete.

    More extensive documentation in bwc.pl is included in the file
    header.

Using bwc.pl with MPI (updated 20160201)
=====================

    0) Think a bit about the block Wiedemann parameters you will use.
    Here, we care mostly about the choice of the parameters m, n,
    interval, mpi, and thr.

    Given N the number of column and rows of your sparse submatrix
    (printed at the end of the replay binary), here are some tips.

     - m and n must be multiples of 64
     - for n=k*64, we do "k sequences". Don't try k>1 before having well
       understood the workflow.
     - the block Wiedemann algorithm will do N/m + 2 * N/n products
       matrix times vector (more precisely, L = N/m + N/n in the krylov
       step, and N/n in the mksol step). Note that for n=k*64, all these
       products are done k times.
     - the linear generator step has time complexity
           O((m+n)*N*log(N)*(m+n+log(N)))
       and space complexity O((m+n)*N). So do not exaggerate with your
       choice of m and n.
     - checkpoints will be saved every [[interval]] iterations. The
       default value is too small. Please set [[interval]] to something
       of your liking. Something like sqrt(L) rounded up to the next
       power of 2, or power of 10, does not seem absurd.
     - a matrix-times-vector product does not scale linearly with mpi=
       and thr=, because of communications. mpi= and thr= are
       two-dimensional parameters, and you'd better have them describe a
       split which should be as close to a square as possible (because
       this minimizes communication costs).

    1) first you have to compile CADO-NFS with MPI support:

    $ rm -rf build/           # make sure to remove any previous non-MPI binary
    $ MPI=1 make
    (or MPI=$HOME/packages/openmpi-1.8.6/ make)

    [ note: you may find it convenient to add to your local.sh the
    line:  if [ "$MPI" ] ; then build_tree="${build_tree}.mpi" ; fi
    so that MPI and non-MPI builds go to different build directories ]

    Check the following lines are present in the make output:

    -- Using MPI C compiler /usr/bin/mpicc
    -- Using MPI C++ compiler /usr/bin/mpic++
    -- Using MPI driver /usr/bin/mpiexec

    2) secondly you need a working directory (wdir) which will be created
       on all machines. It can be empty initially. This directory does
       not need to be shared via NFS, but having it shared doesn't hurt
       either.

    3) run simple tests to make sure everything is right. If you're at
       loss with these, then you'll have a hard time doing the rest.

    $ MPI=1 make -j8 test_hello

    # a simple non-mpi threaded test run
    $ ./build/xxxxxxx/tests/linalg/bwc/test_hello thr=2x2

    # a simple mpi non-threaded test run
    $ /usr/bin/mpiexec -n 4 ./build/xxxxxxx/tests/linalg/bwc/test_hello mpi=2x2

    # combining both
    $ /usr/bin/mpiexec -n 4 ./build/xxxxxxx/tests/linalg/bwc/test_hello mpi=2x2 thr=2x2

    # specify a node file (unique list of machine names in $nodefile)
    $ /usr/bin/mpiexec -n 4 -machinefile $nodefile ./build/xxxxxxx/tests/linalg/bwc/test_hello mpi=2x2 thr=2x2

    # most often, for the previous run to succeed, you have to add
    # some stanzas (here for openmpi and the OAR job scheduler):
    $ /usr/bin/mpiexec -n 4 -machinefile $OAR_NODEFILE --mca rmaps_base_mapping_policy node -mca plm_rsh_agent oarsh ./build/xxxxxxx/tests/linalg/bwc/test_hello  mpi=2x2 thr=4x4

    4) then launch the script ./build/xxxxxxx/linalg/bwc/bwc.pl, as in
    the command line below, where you have to adapt:
       thr=3x4   each machine has 12 cores, we split them in 3x4
       mpi=2x2:  we use 4 machines, and split them in 2x2
       matrix=   location of the matrix (on the machine where bwc.pl is run)
       wdir=...  working directory (accessible by all machines)

    # A simple command-line where we benefit from some auto-guessing for
    # some arcane parameters
    $ ./build/xxxxxxx/linalg/bwc/bwc.pl :complete thr=3x4 mpi=2x2 matrix=/localdisk/tmp/nfs/c156.sparse.bin interval=4096 m=64 n=64 wdir=/tmp/bwc.wdir

    # A rather more ``manual'' run.
    $ ./build/xxxxxxx/linalg/bwc/bwc.pl :complete thr=3x4 mpi=2x2 hosts=andouille,chipolata,chorizo,merguez matrix=/localdisk/tmp/nfs/c156.sparse.bin interval=4096 m=64 n=64 wdir=/global/bwc.wdir bwc_bindir=/global/bin/bwc mpi_extra_args='--mca btl_tcp_if_exclude lo,virbr0'

    There are very many internal parameters, some of which get passed to
    mpiexec, based on some auto-detection, both of the job scheduler and
    the MPI back-end which is used. This works sometimes rather
    awkwardly, so the relevant bits of the linalg/bwc/bwc.pl script may
    have to be edited to best suit your combination (please submit your
    changes to the cado-nfs-discuss mailing list, if you do so).

    Note: Some additional scheduler-specific bits and pieces (consider them as
    example documentation status only -- none are part of regular tests):
    openpbs/torque : linalg/bwc/README.TORQUE

    Note: see also cpu binding below.

If you intend to only use one sequence (n == 64), it's not very
difficult. If you want to do several sequences, you have to split the
:complete meta-step into several, and run them at the appropriate
moments. This also works, but requires you to know a bit what you're
doing.

Cpu binding (updated 20141002)
==============================

If hwloc (Portable Hardware Locality,
http://www.open-mpi.org/projects/hwloc/) is installed and you have a
C++11 compiler, then bwc.pl might use it for cpu binding, which will
improve the performance of the linear algebra step. When hwloc is used,
the keyword "cpubinding" should appear in cxxx.bwc.log.

See cpubinding.conf for more information. In short, you just need to add on the
cado-nfs.py command line: cpubinding=$(CADO)/parameters/misc/cpubinding.conf.

Using Cantor's algorithm
========================

At the highest levels of the recursion tree of the 'lingen' program, one
might use Cantor's algorithm to speed up the computations (see
https://hal.inria.fr/inria-00188261). To use Cantor's algorithm, add for
example tasks.linalg.bwc.cantor_threshold=128 to the cado-nfs.py command line;
this means that Cantor's algorithm will be used to multiply matrices of
polynomials of degree 128 or more. Note that Cantor's algorithm is faster
for large degrees, but uses more memory. The default is
tasks.linalg.bwc.cantor_threshold=2048. To disable Cantor's algorithm, use
tasks.linalg.bwc.cantor_threshold=4294967295.

Using several sequences
=======================

See https://gforge.inria.fr/tracker/index.php?func=detail&aid=16857&group_id=2065&atid=7445 and
https://lists.gforge.inria.fr/pipermail/cado-nfs-discuss/2015-April/000389.html

Notes:
======

(T): This assumes that nullspace=right. If nullspace=left, replace M by trsp(M).
